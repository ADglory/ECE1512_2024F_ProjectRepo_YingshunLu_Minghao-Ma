# -*- coding: utf-8 -*-
"""1512p1t1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Rnz-M1pmozTGbRyu7LS8eiYrQS8CFan
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import matplotlib.pyplot as plt
import zipfile
import os
import pandas as pd
from PIL import Image

# Define ConvNet3 and ConvNet7 Models
class ConvNet3(nn.Module):
    def __init__(self):
        super(ConvNet3, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 28 * 28, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc1(x)
        return x

class ConvNet7(nn.Module):
    def __init__(self, num_classes=2):
        super(ConvNet7, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(256 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Import Libraries and Define Data Transformations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# File Upload and Extraction
#  Make sure the ZIP file has been uploaded to images.zip under the /content/mhist_dataset folder
mhist_zip_path = '/content/mhist_dataset/images.zip'
mhist_extract_path = '/content/mhist_dataset/images'
# unzip MHIST ZIP file
with zipfile.ZipFile(mhist_zip_path, 'r') as zip_ref:
    zip_ref.extractall(mhist_extract_path)

#Load MNIST Dataset for Standard Training
mnist_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)
test_dataset_mnist = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)

train_loader_mnist = DataLoader(mnist_dataset, batch_size=256, shuffle=True)
test_loader_mnist = DataLoader(test_dataset_mnist, batch_size=256, shuffle=False)

# Custom MHIST data set classes
class MHISTDataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file, img_dir, transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = Image.open(img_path).convert("RGB")
        label_text = self.img_labels.iloc[idx, 1]
        # Convert labels to integers 'HP' -> 0, 'SSA' -> 1
        if label_text == 'HP':
            label = 0
        elif label_text == 'SSA':
            label = 1
        else:
            raise ValueError(f"Unexpected label value: {label_text}")

        if self.transform:
            image = self.transform(image)

        return image, label

# MHIST data set path
annotations_file = '/content/mhist_dataset/annotations.csv'
img_dir = '/content/mhist_dataset/images/images'

# Set up preprocessing of MHIST data sets
mhist_transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Suitable for single-channel grayscale images
])

# Load the MHIST data set
mhist_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
train_loader_mhist = DataLoader(mhist_dataset, batch_size=128, shuffle=True)

# Define Model, Loss, and Optimizer for Standard Training
model_mnist = ConvNet3().to(device)
criterion = nn.CrossEntropyLoss()
optimizer_mnist = optim.SGD(model_mnist.parameters(), lr=0.01, momentum=0.9)

# Train the model on full dataset
def train_model(model, data_loader, criterion, optimizer, epochs=20):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for batch_idx, (inputs, labels) in enumerate(data_loader):
            try:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            except Exception as e:
                continue

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(data_loader)}")

def evaluate_model(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Test Accuracy: {100 * correct / total}%")
# For MNIST
train_model(model_mnist, train_loader_mnist, criterion, optimizer_mnist, epochs=20)
evaluate_model(model_mnist, test_loader_mnist)

# Train ConvNet-7 on MHIST Dataset
model_mhist = ConvNet7().to(device)
optimizer_mhist = optim.SGD(model_mhist.parameters(), lr=0.01, momentum=0.9)

train_model(model_mhist, train_loader_mhist, criterion, optimizer_mhist, epochs=20)
evaluate_model(model_mhist, train_loader_mhist)

pip install fvcore

from fvcore.nn import FlopCountAnalysis
import torch



model_convnet3 = ConvNet3().to(device)
model_convnet7 = ConvNet7().to(device)

# FLOPs
input_tensor = torch.randn(1, 1, 28, 28).to(device)
flop_counter_convnet3 = FlopCountAnalysis(model_convnet3, input_tensor)
print(f"ConvNet3 FLOPs: {flop_counter_convnet3.total()}")

flop_counter_convnet7 = FlopCountAnalysis(model_convnet7, input_tensor)
print(f"ConvNet7 FLOPs: {flop_counter_convnet7.total()}")

######################
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

# Initializing distillation images from real data
def initialize_from_real_data(data_loader, num_classes, num_images_per_class):
    """
    从真实数据集中随机选择每个类别的图像来初始化蒸馏图像。
    """
    real_images = []
    real_labels = []
    for class_idx in range(num_classes):
        count = 0
        for images, labels in data_loader:
            mask = (labels == class_idx)
            selected_images = images[mask]
            if selected_images.size(0) > 0:
                real_images.append(selected_images[:num_images_per_class])
                real_labels.extend([class_idx] * min(num_images_per_class, selected_images.size(0)))
                count += selected_images.size(0)
            if count >= num_images_per_class:
                break
    real_images = torch.cat(real_images).to(device)
    real_labels = torch.tensor(real_labels).to(device)
    return real_images, real_labels

# Initialize the distillation image and label for MHIST
num_classes = 2  # Suppose that MHIST is a binary classification problem
num_images_per_class = 10  # The number of distilled images per category
distilled_images, distilled_labels = initialize_from_real_data(train_loader_mhist, num_classes, num_images_per_class)
distilled_images.requires_grad = True
lr_image = 0.1  # Learning rate of distillation image
optimizer_images = optim.SGD([distilled_images], lr=lr_image)

# Initialize the distillation image and label for MNIST
num_classes_MNIST = 10 #0-9
distilled_images_MNIST, distilled_labels_MNIST = initialize_from_real_data(train_loader_mnist, num_classes_MNIST, num_images_per_class)
distilled_images.requires_grad = True
optimizer_images_MNIST = optim.SGD([distilled_images_MNIST], lr=lr_image)

# Data distillation using Attention Matching
def distill_with_attention_matching(model, data_loader, distilled_images, distilled_labels, optimizer_images, T=10):
    """
    Data distillation based on Attention Matching.
    Parameter:
        model: torch.nn.Module
        data_loader: DataLoader
        distilled_images: torch.Tensor
        distilled_labels: torch.Tensor
        optimizer_images: torch.optim.Optimizer
        T: int
    """
    model.eval()
    for t in range(T):
        total_loss = 0.0
        for real_images, real_labels in data_loader:
            real_images, real_labels = real_images.to(device), real_labels.to(device)

            optimizer_images.zero_grad()
            with torch.no_grad():
                real_outputs = model(real_images)
                real_loss = F.cross_entropy(real_outputs, real_labels)

            synthetic_outputs = model(distilled_images)
            synthetic_loss = F.cross_entropy(synthetic_outputs, distilled_labels)
            synthetic_loss.backward()
            optimizer_images.step()

            total_loss += synthetic_loss.item()

        print(f"iter {t+1}/{T}, Distillation loss: {total_loss / len(data_loader)}")

# Data distillation for MHIST
T = 10
distill_with_attention_matching(model_mhist, train_loader_mhist, distilled_images, distilled_labels, optimizer_images, T=T)

# Data distillation for MNIST
distill_with_attention_matching(model_mnist, train_loader_mnist, distilled_images_MNIST, distilled_labels_MNIST, optimizer_images_MNIST, T=T)

# Retrain the model on the distillation data set
def train_on_distilled_data(model, distilled_images, distilled_labels, epochs=20, lr=0.01):
    distilled_dataset = TensorDataset(distilled_images, distilled_labels)
    distilled_loader = DataLoader(distilled_dataset, batch_size=32, shuffle=True)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in distilled_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(distilled_loader)}")

# Retrain the model on the distillation data set FOR MHIST
model_mhist = ConvNet7().to(device)
train_on_distilled_data(model_mhist, distilled_images, distilled_labels, epochs=20, lr=0.01)

# Retrain the model on the distillation data set FOR MNIST
model_mnist = ConvNet3().to(device)
train_on_distilled_data(model_mnist, distilled_images_MNIST, distilled_labels_MNIST, epochs=20, lr=0.01)

# Evaluate model performance on a real-world test set
def evaluate_model(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"Accuracy on a real test set: {100 * correct / total}%")


test_dataset_mhist = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
test_loader_mhist = DataLoader(test_dataset_mhist, batch_size=128, shuffle=False)
# Evaluate the model using test sets for MHIST
evaluate_model(model_mhist, test_loader_mhist)

# Evaluate the model using test sets for MNIST
evaluate_model(model_mnist, test_loader_mnist)

import matplotlib.pyplot as plt

# Displays distillation images for each category
def visualize_distilled_images(distilled_images, num_classes, num_images_per_class):
    fig, axes = plt.subplots(num_classes, num_images_per_class, figsize=(10, 5))
    for class_idx in range(num_classes):
        for img_idx in range(num_images_per_class):
            ax = axes[class_idx, img_idx]
            # The gradient is separated using.detach(), converted to CPU and converted to numpy format
            ax.imshow(distilled_images[class_idx * num_images_per_class + img_idx].cpu().detach().squeeze(), cmap='gray')
            ax.axis('off')
    plt.show()

# Visualize the distillation data set generated by steps 1 and 2
# For MHIST
visualize_distilled_images(distilled_images, num_classes=2, num_images_per_class=num_images_per_class)

#For MNIST
visualize_distilled_images(distilled_images_MNIST, num_classes=10, num_images_per_class=num_images_per_class)

# The distillation image is initialized with Gaussian noise and the distillation is started from scratch
# Gaussian noise initialization was used for comparative experiments
distilled_images_gaussian = torch.randn((2 * num_images_per_class, 1, 28, 28), requires_grad=True, device=device)
distilled_labels_gaussian = torch.tensor([0] * num_images_per_class + [1] * num_images_per_class).to(device)
optimizer_images_gaussian = optim.SGD([distilled_images_gaussian], lr=lr_image)  # Use the same learning rate

# Rerun the distillation process, starting from scratch
T_gaussian = 10
distill_with_attention_matching(model_mhist, train_loader_mhist, distilled_images_gaussian, distilled_labels_gaussian, optimizer_images_gaussian, T=T_gaussian)

# Retrain the model on the distillation data set
model_mhist_gaussian = ConvNet7().to(device)  # Reinitialize the model, making sure to start from scratch
train_on_distilled_data(model_mhist_gaussian, distilled_images_gaussian, distilled_labels_gaussian, epochs=20, lr=0.01)

# Define the DataLoader for the MHIST test set
test_dataset_mhist = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
test_loader_mhist = DataLoader(test_dataset_mhist, batch_size=128, shuffle=False)

# Evaluate
evaluate_model(model_mhist_gaussian, test_loader_mhist)


# Visualize distillation data initialized with Gaussian noise
visualize_distilled_images(distilled_images_gaussian, num_classes, num_images_per_class)

import torch.nn as nn
import torch.optim as optim

# Defining new network architectures LeNet
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(-1, 16 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Training LeNet models using synthetic data sets
def train_on_distilled_data_leNet(model, distilled_images, distilled_labels, epochs=20, lr=0.01):
    distilled_dataset = TensorDataset(distilled_images, distilled_labels)
    distilled_loader = DataLoader(distilled_dataset, batch_size=32, shuffle=True)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in distilled_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(distilled_loader)}")

# Initialize and train the LeNet model
model_leNet = LeNet().to(device)
train_on_distilled_data_leNet(model_leNet, distilled_images, distilled_labels, epochs=20, lr=0.01)

# Evaluate the LeNet model on a real test set
evaluate_model(model_leNet, test_loader_mhist)