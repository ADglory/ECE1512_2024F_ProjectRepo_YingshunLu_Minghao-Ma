# -*- coding: utf-8 -*-
"""1512p1t1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Rnz-M1pmozTGbRyu7LS8eiYrQS8CFan
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import matplotlib.pyplot as plt
import zipfile
import os
import pandas as pd
from PIL import Image

# Step 1: Define ConvNet3 and ConvNet7 Models
class ConvNet3(nn.Module):
    def __init__(self):
        super(ConvNet3, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 28 * 28, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.fc1(x)
        return x

class ConvNet7(nn.Module):
    def __init__(self, num_classes=2):
        super(ConvNet7, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(256 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # Flatten the tensor
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Step 2: Import Libraries and Define Data Transformations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 3: File Upload and Extraction
#  ZIP 文件已上传至 /content/mhist_dataset 文件夹下的 images.zip
mhist_zip_path = '/content/mhist_dataset/images.zip'
mhist_extract_path = '/content/mhist_dataset/images'
# 解压 MHIST ZIP 文件
with zipfile.ZipFile(mhist_zip_path, 'r') as zip_ref:
    zip_ref.extractall(mhist_extract_path)

# Step 4: Load MNIST Dataset for Standard Training
mnist_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)
test_dataset_mnist = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)

train_loader_mnist = DataLoader(mnist_dataset, batch_size=256, shuffle=True)
test_loader_mnist = DataLoader(test_dataset_mnist, batch_size=256, shuffle=False)

# 自定义 MHIST 数据集类
class MHISTDataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file, img_dir, transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = Image.open(img_path).convert("RGB")
        label_text = self.img_labels.iloc[idx, 1]
        # 将标签转换为整数 (例如 'HP' -> 0, 'SSA' -> 1)
        if label_text == 'HP':
            label = 0
        elif label_text == 'SSA':
            label = 1
        else:
            raise ValueError(f"Unexpected label value: {label_text}")

        if self.transform:
            image = self.transform(image)

        return image, label

# MHIST 数据集路径
annotations_file = '/content/mhist_dataset/annotations.csv'
img_dir = '/content/mhist_dataset/images/images'

# 设置 MHIST 数据集的预处理
mhist_transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 适用于单通道的灰度图像
])

# 加载 MHIST 数据集
mhist_dataset = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
train_loader_mhist = DataLoader(mhist_dataset, batch_size=128, shuffle=True)

# Step 5: Define Model, Loss, and Optimizer for Standard Training
model_mnist = ConvNet3().to(device)
criterion = nn.CrossEntropyLoss()
optimizer_mnist = optim.SGD(model_mnist.parameters(), lr=0.01, momentum=0.9)

# Train the model on full dataset
def train_model(model, data_loader, criterion, optimizer, epochs=20):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for batch_idx, (inputs, labels) in enumerate(data_loader):
            try:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            except Exception as e:
                continue

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(data_loader)}")

def evaluate_model(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Test Accuracy: {100 * correct / total}%")

train_model(model_mnist, train_loader_mnist, criterion, optimizer_mnist, epochs=20)
evaluate_model(model_mnist, test_loader_mnist)

# Step 6: Train ConvNet-7 on MHIST Dataset
model_mhist = ConvNet7().to(device)
optimizer_mhist = optim.SGD(model_mhist.parameters(), lr=0.01, momentum=0.9)

train_model(model_mhist, train_loader_mhist, criterion, optimizer_mhist, epochs=20)
evaluate_model(model_mhist, train_loader_mhist)

pip install fvcore

from fvcore.nn import FlopCountAnalysis
import torch

# 假设 ConvNet3 和 ConvNet7 已定义

# Step 1: 定义模型
model_convnet3 = ConvNet3().to(device)
model_convnet7 = ConvNet7().to(device)

# Step 2: 计算并打印 FLOPs
input_tensor = torch.randn(1, 1, 28, 28).to(device)  # 输入张量，假设 MNIST 或 MHIST 图像大小为 1x28x28
flop_counter_convnet3 = FlopCountAnalysis(model_convnet3, input_tensor)
print(f"ConvNet3 FLOPs: {flop_counter_convnet3.total()}")

flop_counter_convnet7 = FlopCountAnalysis(model_convnet7, input_tensor)
print(f"ConvNet7 FLOPs: {flop_counter_convnet7.total()}")

######################
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

# Step 1: 从真实数据中初始化蒸馏图像
def initialize_from_real_data(data_loader, num_classes, num_images_per_class):
    """
    从真实数据集中随机选择每个类别的图像来初始化蒸馏图像。
    """
    real_images = []
    real_labels = []
    for class_idx in range(num_classes):
        count = 0
        for images, labels in data_loader:
            mask = (labels == class_idx)
            selected_images = images[mask]
            if selected_images.size(0) > 0:
                real_images.append(selected_images[:num_images_per_class])
                real_labels.extend([class_idx] * min(num_images_per_class, selected_images.size(0)))
                count += selected_images.size(0)
            if count >= num_images_per_class:
                break
    real_images = torch.cat(real_images).to(device)
    real_labels = torch.tensor(real_labels).to(device)
    return real_images, real_labels

# 初始化蒸馏图像和标签
num_classes = 2  # 假设 MHIST 是二分类问题
num_images_per_class = 10  # 每个类别的蒸馏图像数量
distilled_images, distilled_labels = initialize_from_real_data(train_loader_mhist, num_classes, num_images_per_class)
distilled_images.requires_grad = True
lr_image = 0.1  # 蒸馏图像的学习率
optimizer_images = optim.SGD([distilled_images], lr=lr_image)

# Step 2: 使用 Attention Matching 进行数据蒸馏
def distill_with_attention_matching(model, data_loader, distilled_images, distilled_labels, optimizer_images, T=10):
    """
    基于 Attention Matching 进行数据蒸馏。

    参数:
        model: torch.nn.Module - 在 MHIST 数据集上训练好的模型。
        data_loader: DataLoader - MHIST 数据的 DataLoader。
        distilled_images: torch.Tensor - 蒸馏图像数据。
        distilled_labels: torch.Tensor - 蒸馏图像标签。
        optimizer_images: torch.optim.Optimizer - 用于更新蒸馏图像的优化器。
        T: int - 蒸馏过程的迭代次数。
    """
    model.eval()
    for t in range(T):
        total_loss = 0.0
        for real_images, real_labels in data_loader:
            real_images, real_labels = real_images.to(device), real_labels.to(device)

            optimizer_images.zero_grad()
            with torch.no_grad():
                real_outputs = model(real_images)
                real_loss = F.cross_entropy(real_outputs, real_labels)

            synthetic_outputs = model(distilled_images)
            synthetic_loss = F.cross_entropy(synthetic_outputs, distilled_labels)
            synthetic_loss.backward()
            optimizer_images.step()

            total_loss += synthetic_loss.item()

        print(f"迭代 {t+1}/{T}, 蒸馏损失: {total_loss / len(data_loader)}")

# 进行数据蒸馏
T = 10
distill_with_attention_matching(model_mhist, train_loader_mhist, distilled_images, distilled_labels, optimizer_images, T=T)



# Step 3: 在蒸馏数据集上重新训练模型
def train_on_distilled_data(model, distilled_images, distilled_labels, epochs=20, lr=0.01):
    distilled_dataset = TensorDataset(distilled_images, distilled_labels)
    distilled_loader = DataLoader(distilled_dataset, batch_size=32, shuffle=True)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in distilled_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(distilled_loader)}")

# 在蒸馏数据集上重新训练模型
model_mhist = ConvNet7().to(device)
train_on_distilled_data(model_mhist, distilled_images, distilled_labels, epochs=20, lr=0.01)

# 评估在真实测试集上的模型性能
def evaluate_model(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f"真实测试集上的准确率: {100 * correct / total}%")

# 定义 MHIST 测试集的 DataLoader（如果需要在真实测试集上评估）
test_dataset_mhist = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
test_loader_mhist = DataLoader(test_dataset_mhist, batch_size=128, shuffle=False)
# 使用测试集评估模型
evaluate_model(model_mhist, test_loader_mhist)

import matplotlib.pyplot as plt

# 显示每个类别的蒸馏图像
def visualize_distilled_images(distilled_images, num_classes, num_images_per_class):
    fig, axes = plt.subplots(num_classes, num_images_per_class, figsize=(10, 5))
    for class_idx in range(num_classes):
        for img_idx in range(num_images_per_class):
            ax = axes[class_idx, img_idx]
            # 使用 .detach() 将梯度分离，转换为 CPU 并转换为 numpy 格式
            ax.imshow(distilled_images[class_idx * num_images_per_class + img_idx].cpu().detach().squeeze(), cmap='gray')
            ax.axis('off')
    plt.show()

# 可视化步骤1和步骤2生成的蒸馏数据集
visualize_distilled_images(distilled_images, num_classes=2, num_images_per_class=num_images_per_class)

# Step 4: 用高斯噪声初始化蒸馏图像并从头开始蒸馏
# 符合 Task 1 的要求，用高斯噪声初始化以进行对比实验
distilled_images_gaussian = torch.randn((2 * num_images_per_class, 1, 28, 28), requires_grad=True, device=device)
distilled_labels_gaussian = torch.tensor([0] * num_images_per_class + [1] * num_images_per_class).to(device)
optimizer_images_gaussian = optim.SGD([distilled_images_gaussian], lr=lr_image)  # 使用与 S1-S3 相同的学习率

# 重新进行蒸馏过程，从头开始
T_gaussian = 10  # 可以设置为和 S1-S3 相同的迭代次数
distill_with_attention_matching(model_mhist, train_loader_mhist, distilled_images_gaussian, distilled_labels_gaussian, optimizer_images_gaussian, T=T_gaussian)

# 在蒸馏数据集上重新训练模型
model_mhist_gaussian = ConvNet7().to(device)  # 重新初始化模型，确保从头开始
train_on_distilled_data(model_mhist_gaussian, distilled_images_gaussian, distilled_labels_gaussian, epochs=20, lr=0.01)

# 定义 MHIST 测试集的 DataLoader（如果需要在真实测试集上评估）
test_dataset_mhist = MHISTDataset(annotations_file=annotations_file, img_dir=img_dir, transform=mhist_transform)
test_loader_mhist = DataLoader(test_dataset_mhist, batch_size=128, shuffle=False)

# 使用真实测试集进行评估
evaluate_model(model_mhist_gaussian, test_loader_mhist)


# 可视化用高斯噪声初始化的蒸馏数据
visualize_distilled_images(distilled_images_gaussian, num_classes, num_images_per_class)

import torch.nn as nn
import torch.optim as optim

# 定义新的网络架构（例如 LeNet）
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)  # 根据你的类别数调整输出大小

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(-1, 16 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 使用合成数据集训练 LeNet 模型
def train_on_distilled_data_leNet(model, distilled_images, distilled_labels, epochs=20, lr=0.01):
    distilled_dataset = TensorDataset(distilled_images, distilled_labels)
    distilled_loader = DataLoader(distilled_dataset, batch_size=32, shuffle=True)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in distilled_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(distilled_loader)}")

# 初始化 LeNet 模型并训练
model_leNet = LeNet().to(device)
train_on_distilled_data_leNet(model_leNet, distilled_images, distilled_labels, epochs=20, lr=0.01)

# 在真实测试集上评估 LeNet 模型
evaluate_model(model_leNet, test_loader_mhist)